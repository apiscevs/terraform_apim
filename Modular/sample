# APIM Throughput — Findings (short)

We load‑tested our APIM policy on the **Developer** tier by hammering a single endpoint with **JMeter (500 VUs)**. The backend was mocked in APIM (`return-response`), so the gateway/policies were the only moving parts. The policy validates a multi‑tenant JWT on every request and applies a JSON‑driven rate limit per route/method.

### Results

* **Full policy (JWT + rate limit): \~260 r/s**
* **JWT only (rate limit off): \~295 r/s**  → \~+13%
* **No JWT, no rate limit:** **\~380 r/s** → \~+46% vs full

### What we learned

* **JWT validation is the main bottleneck.** It sets the ceiling on RPS.
* **Rate limiting adds a bit of overhead**, but it’s small next to JWT.
* Caching the route→group lookup (and normalizing paths) is **clean and correct**, but doesn’t materially change throughput under load.
* APIM **Capacity** is a composite “how busy” index. Managed APIM doesn’t expose raw CPU/RAM; Capacity + Gateway vs Backend latency are the useful signals here.

### Test setup (for posterity)

* Tier: **APIM Developer** (single unit)
* Tool: **JMeter**, 500 virtual users, steady spam of one endpoint
* Backend: **mocked in APIM** to isolate gateway cost
* Policy bits: **`validate-jwt` every request**, custom **rate limit** (per route/method), route→group **cache** keyed by **revision + method + normalized path** (`calls|period|name` packed string)

That’s the gist: if we keep JWT validation in APIM for every call, micro‑optimizations won’t move the needle; higher RPS needs backend JWT or a bigger tier.
